{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_parsivar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCO0CKboE1hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install parsivar "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoYAnlEHXT6f",
        "colab_type": "text"
      },
      "source": [
        "<html>\n",
        "  <h3>\n",
        "    I've run SQL files in my LopTop, convert tables to .csv and uploaded them to my google drive. At cell bellow i've read them from drive .\n",
        "  </h3>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px4e4jqKFETA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "social = pd.read_csv('/content/drive/My Drive/parto tech/social.csv')\n",
        "economy = pd.read_csv('/content/drive/My Drive/parto tech/economy.csv')\n",
        "politics = pd.read_csv('/content/drive/My Drive/parto tech/politics.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH7ZK7Y6FUf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "al = economy.append(social,ignore_index=True)\n",
        "al = al.append(politics,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGAUgXCbFZFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_backslashes(mystring) :\n",
        "  ret =  mystring.replace('\\\\n',' ')\n",
        "  ret =  ret.replace('\\\\r\\\\n',' ')\n",
        "  ret =  ret.replace('\\\\n\\\\r',' ')\n",
        "  ret =  ret.replace('\\n',' ')\n",
        "  ret =  ret.replace('\\r\\n',' ')\n",
        "  ret =  ret.replace('\\n\\r',' ')\n",
        "  ret =  ret.replace('\\r',' ')\n",
        "  return ret.replace('\\\\r',' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBOaJtR7Fc_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "al.loc[:]['train_text'] = al['train_text'].apply(remove_backslashes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvWvRaEsXhUM",
        "colab_type": "text"
      },
      "source": [
        "<html>\n",
        "  <h3>\n",
        "    Till this step i read csv files, append them together then apply <i><b>remove_backslashes</b></i> function to <i>train_text</i> column\n",
        "  </h3>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_y1J0HFgMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report ,f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVGcGKrbGNh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from parsivar import * "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTlM4kCULJAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "stop_words = pickle.load(open('/content/drive/My Drive/parto tech/hazm_stopwords.pkl','rb'))\n",
        "stop_words = stop_words.split(',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oavmhKOXmZ8",
        "colab_type": "text"
      },
      "source": [
        "<html>\n",
        "  <h3>Cell below contains three steps :</h3>\n",
        "  <ol>\n",
        "    <li>Defining a function for preprocessing the text. First i extract sentences then iterate for each sentence and normalize, lemmatize and stem all words in a senctence. </li>\n",
        "    <li>Creating pipeline of Count vectorizer, Tfidf transformer and model</li>\n",
        "    <li>Fitting the model</li>\n",
        "  </ol>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og08nhteFjvh",
        "colab_type": "code",
        "outputId": "e5fe6d83-e770-4a17-f4ad-d728b154654e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def preprocessing(text) :\n",
        "  normalizer = Normalizer()\n",
        "  tokenizer = Tokenizer()\n",
        "  stemmer = FindStems()\n",
        "\n",
        "  all_sentences = tokenizer.tokenize_sentences(text) \n",
        "  new_sents = [] \n",
        "\n",
        "  for sent in all_sentences :\n",
        "    all_words = tokenizer.tokenize_words(normalizer.normalize(sent)) \n",
        "    new_sent = [ stemmer.convert_to_stem(word).split('&')[0] for word in all_words if word not in stop_words]\n",
        "    new_sent = ' '.join(new_sent)\n",
        "    new_sents.append(new_sent)\n",
        "\n",
        "  return ' '.join(new_sents)\n",
        "\n",
        "pipline = Pipeline([\n",
        "        ('bag of words', CountVectorizer(preprocessor=preprocessing)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('model', KNeighborsClassifier(n_neighbors=2))\n",
        "    ])\n",
        "\n",
        "msg_train, msg_test, label_train, label_test = train_test_split(al['train_text'], al['cat_id'], test_size=0.3,\n",
        "                                                                    random_state=101)\n",
        "\n",
        "pipline.fit(msg_train, label_train)\n",
        "print(pipline.score(msg_test, label_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9854014598540146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B069A0kAVTzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = pipline.predict(msg_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6WeQRmGVWRF",
        "colab_type": "code",
        "outputId": "b41244bb-0b65-4e1f-bd28-a576873242ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "print(classification_report(y_true=label_test,y_pred=y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           3       1.00      0.94      0.97        18\n",
            "           5       0.98      1.00      0.99        52\n",
            "           7       0.99      0.99      0.99        67\n",
            "\n",
            "    accuracy                           0.99       137\n",
            "   macro avg       0.99      0.98      0.98       137\n",
            "weighted avg       0.99      0.99      0.99       137\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azOQr7eiVdF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "\n",
        "file_name = '/content/drive/My Drive/parto tech/model_parsivar.pkl'\n",
        "pickle.dump(pipline,open(file_name,'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}