{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_hazm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1uvajl_k6Xr",
        "colab_type": "code",
        "outputId": "20f9c15c-f0f1-470f-c842-eea1c70a3f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "!pip install hazm\n",
        "import pandas as pd \n",
        "from hazm import * "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.8MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 6.2MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.12.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394471 sha256=f03af7db5ec1109b3db0407fc751a0dbaa9d2b766d4224a4b248dfe1245beaaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=153524 sha256=66d8fb74bb29696ca94298c36cdd683ff896b959f3c3a5af23fd882dc9e44c45\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPSMvdTC5Ei6",
        "colab_type": "text"
      },
      "source": [
        "<html>\n",
        "  <h3>\n",
        "    I've run SQL files in my LopTop, convert tables to .csv and uploaded them to my google drive. At cell bellow i've read them from drive .\n",
        "  </h3>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNma7hUsljrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "social = pd.read_csv('/content/drive/My Drive/parto tech/social.csv')\n",
        "economy = pd.read_csv('/content/drive/My Drive/parto tech/economy.csv')\n",
        "politics = pd.read_csv('/content/drive/My Drive/parto tech/politics.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKXEnVJdU751",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "al = economy.append(social,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcul1oG7VIdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "al = al.append(politics,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MNKVEJfdmYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_backslashes(mystring) :\n",
        "  ret =  mystring.replace('\\\\n',' ')\n",
        "  ret =  ret.replace('\\\\r\\\\n',' ')\n",
        "  ret =  ret.replace('\\\\n\\\\r',' ')\n",
        "  ret =  ret.replace('\\n',' ')\n",
        "  ret =  ret.replace('\\r\\n',' ')\n",
        "  ret =  ret.replace('\\n\\r',' ')\n",
        "  ret =  ret.replace('\\r',' ')\n",
        "  return ret.replace('\\\\r',' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a03nakfykZtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "al.loc[:]['train_text'] = al['train_text'].apply(remove_backslashes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0WCBvsP4E1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "al.to_pickle('/content/drive/My Drive/parto tech/all_together.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnDr2rNp6-2a",
        "colab_type": "text"
      },
      "source": [
        "<html>\n",
        "  <h3>\n",
        "    Till this step i read csv files, append them together then apply <i><b>remove_backslashes</b></i> function to <i>train_text</i> column\n",
        "  </h3>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzUWfkp9XXHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report ,f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ95uhDY8ZAV",
        "colab_type": "text"
      },
      "source": [
        "<html>\n",
        "  <h3>Cell below contains three steps :</h3>\n",
        "  <ol>\n",
        "    <li>Defining a function for preprocessing the text. First i extract sentences then iterate for each sentence and normalize, lemmatize and stem all words in a senctence. </li>\n",
        "    <li>Creating pipeline of Count vectorizer, Tfidf transformer and model</li>\n",
        "    <li>Fitting the model</li>\n",
        "  </ol>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2tMFM58YNgZ",
        "colab_type": "code",
        "outputId": "44c279b1-56a4-40a3-e8bf-a84b4ec21580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def preprocessing(text) :\n",
        "  stemmer = Stemmer()\n",
        "  lemmatizer = Lemmatizer()\n",
        "  normalizer = Normalizer()\n",
        "\n",
        "  stop_words = stopwords_list()\n",
        "  all_sentences = sent_tokenize(text)\n",
        "  new_sents = []\n",
        "  for sent in all_sentences :\n",
        "    all_words = word_tokenize(normalizer.normalize(sent))\n",
        "    new_sent = [lemmatizer.lemmatize(stemmer.stem(word)).split('#')[0] for word in all_words if word not in stop_words]\n",
        "    new_sent = ' '.join(new_sent) \n",
        "    new_sents.append(new_sent)\n",
        "  \n",
        "  return ' '.join(new_sents)\n",
        "\n",
        "\n",
        "pipline = Pipeline([\n",
        "        ('bag of words', CountVectorizer(preprocessor=preprocessing)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('model', KNeighborsClassifier(n_neighbors=2))\n",
        "    ])\n",
        "\n",
        "msg_train, msg_test, label_train, label_test = train_test_split(al['train_text'], al['cat_id'], test_size=0.3,\n",
        "                                                                    random_state=101)\n",
        "\n",
        "pipline.fit(msg_train, label_train)\n",
        "print(pipline.score(msg_test, label_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9854014598540146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX7WHWBqxS3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = pipline.predict(msg_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC_ZRxQgxszE",
        "colab_type": "code",
        "outputId": "ac1d8ffe-12a3-45ef-ea6d-33e33fbbd7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "print(classification_report(y_true=label_test,y_pred=y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           3       1.00      0.94      0.97        18\n",
            "           5       0.98      1.00      0.99        52\n",
            "           7       0.99      0.99      0.99        67\n",
            "\n",
            "    accuracy                           0.99       137\n",
            "   macro avg       0.99      0.98      0.98       137\n",
            "weighted avg       0.99      0.99      0.99       137\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_CeqVJT2Miy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle \n",
        "\n",
        "file_name = '/content/drive/My Drive/parto tech/model_hazm.pkl'\n",
        "pickle.dump(pipline,open(file_name,'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}